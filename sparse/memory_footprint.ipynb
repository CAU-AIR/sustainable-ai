{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_memory_usage(model, input_size, optimizer):\n",
    "    # 모델을 GPU로 옮기기\n",
    "    model = model.cuda()\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "\n",
    "    # 모델 가중치 메모리 계산\n",
    "    total_weights_memory = sum(p.element_size() * p.nelement() for p in model.parameters())\n",
    "\n",
    "    # 더미 입력 데이터 생성\n",
    "    inputs = torch.randn(input_size).cuda()\n",
    "\n",
    "    # 활성화 메모리 계산을 위한 훅 등록\n",
    "    activations = []\n",
    "    def save_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations.append(output.element_size() * output.nelement())\n",
    "        return hook\n",
    "\n",
    "    for name, layer in model.named_modules():\n",
    "        layer.register_forward_hook(save_activation(name))\n",
    "\n",
    "    # 순전파 실행\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # 손실 계산 및 역전파 준비\n",
    "    criterion = nn.MSELoss()\n",
    "    labels = torch.randn(outputs.shape).cuda()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    # 기울기 메모리 계산\n",
    "    gradients_memory = sum(p.grad.element_size() * p.grad.nelement() for p in model.parameters() if p.grad is not None)\n",
    "\n",
    "    # 옵티마이저 상태 메모리 계산을 위해 step 실행\n",
    "    optimizer.step()\n",
    "\n",
    "    # 옵티마이저 상태 메모리 계산\n",
    "    optimizer_state_memory = 0\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                optimizer_state_memory += v.element_size() * v.nelement()\n",
    "\n",
    "    # 메모리 사용량 출력\n",
    "    total_activations_memory = sum(activations)\n",
    "    print(f'Total weight memory: {total_weights_memory / (1024 ** 2):.2f} MB')\n",
    "    print(f'Total activation memory: {total_activations_memory / (1024 ** 2):.2f} MB')\n",
    "    print(f'Total gradient memory: {gradients_memory / (1024 ** 2):.2f} MB')\n",
    "    print(f'Total optimizer state memory: {optimizer_state_memory / (1024 ** 2):.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total weight memory: 44.59 MB\n",
      "Total activation memory: 33.51 MB\n",
      "Total gradient memory: 44.59 MB\n",
      "Total optimizer state memory: 44.59 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/anaconda3/envs/ohs/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "# ResNet18 모델 및 옵티마이저 생성\n",
    "model = models.resnet18()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 입력 데이터 크기 설정 (예: 1개의 이미지, 3채널, 224x224)\n",
    "input_size = (1, 3, 224, 224)\n",
    "\n",
    "estimate_memory_usage(model, input_size, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ohs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
